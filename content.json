{"posts":[{"title":"Dynamic Programming","text":"Resources Dynamic Programming 1D (NeetCode) Dynamic Programming 2D (NeetCode) Dynamic Programming Introduction (Abdul Bari)","link":"/2025/09/14/Dynamic-Programming/"},{"title":"AXI","text":"","link":"/2025/09/14/AXI/"},{"title":"Git Basics","text":"","link":"/2024/08/12/Git/"},{"title":"GoogleTPU","text":"","link":"/2025/09/14/GoogleTPU/"},{"title":"LLVM Components","text":"OverviewLLVM is a modular toolchain for building compilers, linkers, and related tooling. Its core idea is a common, language‑independent Intermediate Representation (LLVM IR) plus a rich library of reusable components for optimization and code generation. This blog introduces the major building blocks and how they fit together. Front Ends Clang: C, C++, Objective‑C, CUDA/HIP, OpenMP; lowers source into LLVM IR. Flang: Fortran front end built on the LLVM ecosystem. Others: Rust, Swift, Julia, Zig, and many more use LLVM as a backend. Front ends typically perform parsing, semantic analysis, and lowering to LLVM IR (SSA form). LLVM IR and Bitcode LLVM IR: Typed, Static Single Assignment (SSA) intermediate form organized as Module → Functions → Basic Blocks → Instructions. Text vs Bitcode: IR can be stored as human‑readable .ll or binary .bc (bitcode) for fast loading and LTO. Verifier: Validates structural correctness of IR. Passes and Optimizer Passes: Modular transformations or analyses (e.g., InstCombine, GVN, SCCP, Dead Code Elim, Inliner). Pass Managers: Orchestrate pass pipelines (function, module, CGSCC, loop levels; new PM is default). Vectorization: Loop Vectorize, SLP Vectorize. IPO/LTO: Inter‑procedural optimizations; LTO and ThinLTO optimize across translation units using bitcode. Code Generation (Back End) Instruction Selection: Maps IR to target instructions (SelectionDAG or GlobalISel on newer targets). Register Allocation: Assigns virtual registers to physical (Greedy, PBQP, etc.). Scheduling &amp; Peepholes: Reorder and tighten instruction streams. Prologue/Epilogue &amp; Frame Lowering: Stack frame management and calling convention lowering. Emission: Produces assembly, object files, or machine code buffers. Target Descriptions TableGen: Declarative target specs (instructions, registers, patterns) that generate C++ code. Subtarget Features: CPU/arch variants (e.g., x86-64, skylake, avx2) toggled via attributes/flags. TargetLowering: Hooks controlling how IR ops lower to target instructions. MC Layer (Assembler/Disassembler/Object) Assembler/Disassembler: Target‑independent framework for parsing/printing machine code. Object Emission: Writes ELF/COFF/Mach‑O with relocations and metadata. Tools built on MC: llvm-objdump, llvm-objcopy, llvm-readelf, llvm-nm, etc. Linkers and LTO lld: LLVM’s high‑performance linker (ELF/COFF/Mach‑O flavors). Gold/ld plugins: Enable (Thin)LTO by passing LLVM bitcode to the optimizer at link time. Runtime and C++ Libraries compiler‑rt: Builtins and sanitizers (ASan, UBSan, TSan, etc.). libc++ / libc++abi / libunwind: C++ standard library, ABI, and unwinder. OpenMP (libomp): Runtime for OpenMP offload and threading. Key Developer Tools clang: Front end driver for C/C++/Obj‑C. opt: Run and experiment with optimization passes on IR. llc: Lower IR to native assembly/object. llvm-as / llvm-dis: IR assembler/disassembler (.ll ↔ .bc). tblgen: Processes TableGen descriptions for targets/diagnostics. Typical Pipeline (C → x86‑64) Parse &amp; Lower: Clang parses C/C++ and lowers to LLVM IR. Optimize: opt/Clang pass pipeline runs analyses and transforms (inline, simplify, vectorize…). Codegen: Instruction selection, register allocation, scheduling. Object/Link: Emit .o, link with lld (optionally ThinLTO), produce executable. Where to Explore Next Browse IR (clang -S -emit-llvm foo.c -o foo.ll) and run passes (opt -passes=...). Compare back ends (llc -mtriple=) and try feature flags (e.g., -mattr=+avx2). Inspect objects with llvm-objdump -d and llvm-readelf -S. This overview should give you a map of the LLVM landscape—front ends generate IR, passes optimize it, and back ends turn it into efficient machine code, all powered by reusable libraries and tools.","link":"/2025/08/12/LLVM/"},{"title":"NPU","text":"","link":"/2025/09/14/NPUs/"}],"tags":[{"name":"Software Engineering","slug":"Software-Engineering","link":"/tags/Software-Engineering/"},{"name":"Compiler","slug":"Compiler","link":"/tags/Compiler/"}],"categories":[{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"}],"pages":[{"title":"Murad Qasaimeh","text":"AboutI’m Murad Qasaimeh. I build AI compilers and machine learning systems.I’m intrested about turning research into fast, production‑ready software. AI compilers and graph optimizations. Performance engineering and acceleration. ML systems and developer tooling. Explore Projects Blog Tutorials Publications Curriculum Vitae Tags Connect LinkedIn GitHub Google Scholar","link":"/index.html"},{"title":"Curriculum","text":"CareerMTS Software Engineer, AMD (Full‑time) Location: San Jose, California, United States Dates: Aug 2022 – Present Highlights: Enabling development of compilers, simulators, and performance analysis tools for AI/ML engines. Developing an end‑to‑end compiler for ML frameworks targeting AMD AIE devices using LLVM and MLIR. Lead Software Engineer, Cadence Design Systems (Full‑time) Location: San Jose, California, United States Dates: Sep 2020 – Jul 2022 (1 yr 11 mos) Research Assistant, Iowa State University Location: Ames, Iowa Dates: Sep 2015 – Jun 2020 (4 yrs 10 mos) Research Engineer Intern, Xilinx Location: San Jose, California Dates: May 2018 – Nov 2018 (7 mos) Education Iowa State University – Ames, Iowa PhD in Electrical and Computer Engineering, GPA: 3.85 (2015 – 2020) Thesis: Efficient Processing of Computer Vision and Deep Learning on FPGAs American University of Sharjah – Sharjah, UAE Master in Electrical and Computer Engineering, GPA: 3.82 (2012 – 2014) Thesis: An FPGA-based Parallel Hardware Architecture for Real-time Image Classification Jordan University of Science and Technology – Irbid, Jordan Bachelor of Science in Computer Engineering, GPA: 85.7/100 (2006 – 2012) Senior Design Project: Indoor Mobile Robot Localization and Navigation SkillsProgramming LanguageI often program using C/C++, CUDA, CMake, and Python. I also have experience in C#, Java at least one large project with each. Machine Learning and Deep Learning Natural Language Processing Computer Vision Speech Processing Statistical Methods Optimization Methods","link":"/Curriculum/index.html"},{"title":"Projects","text":"List of projects: Structured Pruning for Deep Neural Networks (DNNs): Implemented magnitude-based structured weight pruning method to improve the performance of its hardware implementation, by keeping number of NNZ values per channel fixed to help load balancing. (Python, Tensorflow). CUDA Implementation of Sparse DNNs: Implemented a high-performance CUDA implementation of structurly sparse DNN and compared its performance with NVIDIA libraries for dense and sparse DNNs: (cuDNN, cuBLAS and cuSPARSE). (C++, CUDA). Benchmarking Vision Kernels and Neural Network Inference Accelerators on Embedded Platforms [1]: Conducted comprehensive benchmarks of accuracy, run-time, and energy efficiency of a wide range of vision kernels and neural networks on multiple embedded platforms: ARM57 CPU, Nvidia Jetson TX2 GPU and Xilinx ZCU102 FPGA. (xFOpenCV, OpenCV, VisionWorks)","link":"/Projects/index.html"},{"title":"PUBLICATIONS","text":"Publications","link":"/Publications/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Tutorials","text":"Quick links to tutorial categories: AI Compilers: /categories/AI-Compilers/ LeetCode: /categories/LeetCode/ Hardware Architecture: /categories/Hardware-Architecture/","link":"/tutorials/index.html"}]}